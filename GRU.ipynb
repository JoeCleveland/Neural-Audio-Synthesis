{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this to the location of the NSynth dataset on your machine\n",
    "files = os.listdir('/media/data/nsynth-train/audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN = 256 #Length of the audio files\n",
    "FILES = 6 #Number of files to use in a batch\n",
    "SEGMN = 1 #Number of segments to take per file\n",
    "BATCH = FILES * SEGMN #Batch size\n",
    "START = 3000 #Start location of the audio \n",
    "NF = 0.05 #'Noise floor' i.e. amount of noise to add for regularization\n",
    "BITR = 64 #Bit rate used for crossentropy loss / mu-law encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.zeros(BATCH, LEN)\n",
    "Param = torch.zeros(BATCH, 1)\n",
    "\n",
    "for f in range(FILES):\n",
    "    #Change this to the location of the NSynth dataset on your machine\n",
    "    sound, sr = torchaudio.load('/media/data/nsynth-train/audio/brass_acoustic_037-0'+str(f*2+60)+'-100.wav')\n",
    "    for b in range(SEGMN):\n",
    "        #Normalize the audio from 0 to 1\n",
    "        clip = sound[:, START:START+LEN]\n",
    "        clip = clip - clip.min() + NF*2\n",
    "        clip = clip / (clip.max() + NF*2)\n",
    "\n",
    "        X[f*SEGMN + b] = clip\n",
    "        Param[f*SEGMN + b] = torch.tensor(f/FILES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, statesize, outsize, paramsize):\n",
    "        super(Network, self).__init__()\n",
    "        self.statesize = statesize\n",
    "        \n",
    "        self.gateLayer1 = nn.GRUCell(1 + paramsize, statesize)\n",
    "        self.linOut = nn.Linear(statesize, outsize)   \n",
    "    \n",
    "    def forward(self, inpt, state, param):\n",
    "        \n",
    "        newstate1 = self.gateLayer1(torch.cat((inpt, param), 1), state[:, 0])\n",
    "        output = self.linOut(newstate1)\n",
    "        \n",
    "        return output, newstate1.view(-1, 1, self.statesize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mu-Law encoding is used for compressing the audio\n",
    "def muLaw(tensor):\n",
    "    return torch.log(1 + (BITR-1)*torch.abs(tensor)) / np.log(BITR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE = 40 #Size of the hidden state\n",
    "\n",
    "rnn = Network(STATE, BITR, 1)\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEPOCH = 100\n",
    "\n",
    "for epoch in range(0, NEPOCH):\n",
    "    \n",
    "    state = torch.zeros(BATCH, 1, STATE)\n",
    "    output = torch.zeros(BATCH, 1)\n",
    "    loss = 0\n",
    "    \n",
    "    for step in range(1, X.shape[1]):\n",
    "        \n",
    "        inpt = muLaw(X[:, step-1]).view(-1, 1)\n",
    "        \n",
    "        #Add random noise for regularization\n",
    "        inpt += torch.randn(1).view(-1, 1) * NF\n",
    "\n",
    "        output, state = rnn(inpt, state, Param)\n",
    "        \n",
    "        #Use either crossentropy loss, or uncomment the next line\n",
    "        #In order to use Mean Squared Error instead\n",
    "        \n",
    "        loss += F.cross_entropy(output, (X[:, step] * (BITR-1)).long())\n",
    "        #loss += F.mse_loss(output, X[:, step])\n",
    "        \n",
    "        #We only perform backpropgation a certain number of steps\n",
    "        #This is known as 'Truncated back-propgation through time'\n",
    "        #Which makes it much easier to train RNNs on long sequences\n",
    "        #In this case we run backprop every 32 steps\n",
    "        if step % 32 == 31:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            state = state.detach()    \n",
    "            output = output.detach()\n",
    "            print(epoch, \"::\", loss/32)\n",
    "            loss = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is used to generate audio from the trained network\n",
    "\n",
    "\n",
    "#Move the network from gpu to cpu\n",
    "rnn_c = rnn.cpu()\n",
    "\n",
    "with torch.no_grad():\n",
    "    state = torch.zeros(1, 4, STATE)\n",
    "    #factor is the number of times to repeat audio generation\n",
    "    factor = 5\n",
    "\n",
    "    output = torch.zeros(1, 1)\n",
    "    outputs = torch.zeros(X.shape[1] * factor, 1)\n",
    "\n",
    "    for step in range(0, X.shape[1] * factor):\n",
    "        inpt = muLaw((torch.argmax(output, dim=1).float()/BITR).view(-1, 1))\n",
    "\n",
    "        #pm is the vector containing pitch information\n",
    "        pm = torch.tensor([step / (X.shape[1] * factor)]).view(1, 1)\n",
    "        output, state = rnn_c(inpt, state, pm)\n",
    "\n",
    "        outputs[step] = (torch.argmax(output, dim=1).float()/BITR).view(-1, 1)\n",
    "\n",
    "    plt.plot(outputs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output the generated audio\n",
    "import wave\n",
    "oput = outputs.detach().numpy()\n",
    "w = wave.open('output_1_low.wav', 'wb')\n",
    "w.setparams((1, 2, 16000, oput.shape[0], 'NONE', 'NONE'))\n",
    "w.writeframes((oput * 30000).astype('int16'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
